{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Collection Notebook**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "\n",
    "* download and clean dataset from Kaggle\n",
    "* save data downloaded from Kaggle in the dataset directory, inputs/cherry_leaves_data\n",
    "\n",
    "### Inputs\n",
    "\n",
    "* Kaggle JSON file - authentication token\n",
    "\n",
    "### Outputs\n",
    "\n",
    "* Dataset stored at inputs/cheer_leaves_data\n",
    "\n",
    "### Insights | Conclusions\n",
    "* The dataset has a data type of \"image\".\n",
    "* the dataset is balanced with both classes containing 2041 pictures.\n",
    "* the dataset was clean, the data cleaning algorithm found no non images\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change working directory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will change the working directory from its current folder to its parent folder using the method os.cetcwd.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/showMeMildew/jupiter_notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "working_dir = os.getcwd()\n",
    "working_dir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we make the parent of our current directory to the current directory\n",
    "*  we pass os.path.dirname() as an arguement which gets the parent directory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(working_dir))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to confirm we have changed our directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/showMeMildew'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "working_dir = os.getcwd()\n",
    "working_dir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch data from kaggle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /workspace/.pip-modules/lib/python3.8/site-packages (1.5.13)\n",
      "Requirement already satisfied: python-dateutil in /home/gitpod/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: tqdm in /workspace/.pip-modules/lib/python3.8/site-packages (from kaggle) (4.65.0)\n",
      "Requirement already satisfied: certifi in /home/gitpod/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from kaggle) (2022.12.7)\n",
      "Requirement already satisfied: requests in /home/gitpod/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from kaggle) (2.28.1)\n",
      "Requirement already satisfied: python-slugify in /workspace/.pip-modules/lib/python3.8/site-packages (from kaggle) (8.0.1)\n",
      "Requirement already satisfied: six>=1.10 in /workspace/.pip-modules/lib/python3.8/site-packages (from kaggle) (1.15.0)\n",
      "Requirement already satisfied: urllib3 in /home/gitpod/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from kaggle) (1.26.13)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /workspace/.pip-modules/lib/python3.8/site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/gitpod/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from requests->kaggle) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/gitpod/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from requests->kaggle) (2.1.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "! pip install kaggle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will change the kaggle configuration directory to current working directory and the permission of kaggle authentication json so the token is recognized in the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['KAGGLE_CONFIG_DIR'] = os.getcwd()\n",
    "! chmod 600 kaggle.json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define the dataset path - The path is the text that follows [https://kaggle.com/datasets/](https://kaggle.com/datasets/) in the url\n",
    "* Define the destination folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "KaggleDatasetPath = \"codeinstitute/cherry-leaves\"\n",
    "DestinationFolder = \"inputs/cherry_leaves_data\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download dataset from kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading cherry-leaves.zip to inputs/cherry_leaves_data\n",
      " 95%|███████████████████████████████████▉  | 52.0M/55.0M [00:01<00:00, 41.1MB/s]\n",
      "100%|██████████████████████████████████████| 55.0M/55.0M [00:01<00:00, 30.1MB/s]\n"
     ]
    }
   ],
   "source": [
    "! kaggle datasets download -d {KaggleDatasetPath} -p {DestinationFolder}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzip and delete the download file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(DestinationFolder + '/cherry-leaves.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall(DestinationFolder)\n",
    "\n",
    "os.remove(DestinationFolder + '/cherry-leaves.zip')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-03 17:01:10.158999: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-03 17:01:10.159039: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find and delete any non-image files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(my_data_dir):\n",
    "    '''\n",
    "    assess all content in the directory provided in the\n",
    "    parameter and delete all non applicable files.\n",
    "\n",
    "    '''\n",
    "    folders = os.listdir(my_data_dir) \n",
    "    for folder in folders:\n",
    "        folder_path = os.path.join(my_data_dir, folder)\n",
    "        files = os.listdir(folder_path)\n",
    "        \n",
    "        total_non_image_files = 0\n",
    "        total_image_files = 0\n",
    "        for given_file in files:\n",
    "            file_location = os.path.join(my_data_dir, folder, given_file)\n",
    "            try:\n",
    "                fobj = open(file_location, \"rb\")\n",
    "                is_jfif = tf.compat.as_bytes(\"JFIF\") in fobj.peek(10)\n",
    "            finally:\n",
    "                fobj.close()\n",
    "\n",
    "            if not is_jfif:\n",
    "                os.remove(file_location)\n",
    "                total_non_image_files += 1\n",
    "            else:\n",
    "                total_image_files += 1\n",
    "                pass\n",
    "        print(f\"Folder: {folder} - has total image file of \",total_image_files)\n",
    "        print(f\"Folder: {folder} - total non-image file deleted is \",total_non_image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder: healthy - has total image file of  2104\n",
      "Folder: healthy - total non-image file deleted is  0\n",
      "Folder: powdery_mildew - has total image file of  2104\n",
      "Folder: powdery_mildew - total non-image file deleted is  0\n"
     ]
    }
   ],
   "source": [
    "data_cleaning('inputs/cherry_leaves_data/cherry-leaves/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data set into train, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['healthy', 'powdery_mildew']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = os.listdir(\"inputs/cherry_leaves_data/cherry-leaves/\")\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import random\n",
    "\n",
    "def split_dataset(my_data_dir, train_set_ratio, validation_set_ratio, test_set_ratio):\n",
    "  '''Source code from \n",
    "  [code institute -data preparation](https://github.com/GyanShashwat1611/WalkthroughProject01)\n",
    "\n",
    "\n",
    "  divides the dataset into train, test and validation datasets using the ratios\n",
    "  provided in the parameters. each split is put into a different folder containing \n",
    "  another two folders for both healthy and powdery mildew.\n",
    "\n",
    "  '''\n",
    "  if train_set_ratio + validation_set_ratio + test_set_ratio != 1.0:\n",
    "    print(\"train_set_ratio + validation_set_ratio + test_set_ratio should sum 1.0\")\n",
    "    return\n",
    "\n",
    "  # gets classes labels\n",
    "  labels = os.listdir(my_data_dir) # it should get only the folder name\n",
    "  if 'test' in labels:\n",
    "    pass\n",
    "  else: \n",
    "    # create train, test folders with classess labels sub-folder\n",
    "    for folder in ['train','validation','test']:\n",
    "      for label in labels:\n",
    "        os.makedirs(name=my_data_dir+ '/' + folder + '/' + label)\n",
    "\n",
    "    for label in labels:\n",
    "\n",
    "      files = os.listdir(my_data_dir + '/' + label)\n",
    "      random.seed(110)\n",
    "      random.shuffle(files)\n",
    "\n",
    "      train_set_files_qty = int(len(files) * train_set_ratio)\n",
    "      validation_set_files_qty = int(len(files) * validation_set_ratio)\n",
    "\n",
    "      count = 1\n",
    "      for file_name in files:\n",
    "        if count <= train_set_files_qty:\n",
    "          # move given file to train set\n",
    "          shutil.move(my_data_dir + '/' + label + '/' + file_name,\n",
    "                      my_data_dir + '/train/' + label + '/' + file_name)\n",
    "          \n",
    "\n",
    "        elif count <= (train_set_files_qty + validation_set_files_qty ):\n",
    "          # move given file to validation set\n",
    "          shutil.move(my_data_dir + '/' + label + '/' + file_name,\n",
    "                      my_data_dir + '/validation/' + label + '/' + file_name)\n",
    "\n",
    "        else:\n",
    "          # move given file to test set\n",
    "          shutil.move(my_data_dir + '/' + label + '/' + file_name,\n",
    "                  my_data_dir + '/test/' +label + '/'+ file_name)\n",
    "          \n",
    "        count += 1\n",
    "\n",
    "      os.rmdir(my_data_dir + '/' + label)\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split the dataset into the following ratios\n",
    "\n",
    "* 0.7 for the train dataset\n",
    "* 0.2 for the test dataset\n",
    "* 0.1 for the validation dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset(my_data_dir = f\"inputs/cherry_leaves_data/cherry-leaves\",\n",
    "                        train_set_ratio = 0.7,\n",
    "                        validation_set_ratio=0.1,\n",
    "                        test_set_ratio=0.2\n",
    "                        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next step"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
